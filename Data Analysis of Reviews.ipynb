{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c3493b",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8874f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efe0ee",
   "metadata": {},
   "source": [
    "#### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad99b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6448, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>date</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was very impressed with the resort.\\n Great ...</td>\n",
       "      <td>2019/08/20</td>\n",
       "      <td>Sebastian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The rooms were nice the outside needs work als...</td>\n",
       "      <td>2019/08/20</td>\n",
       "      <td>Los Angeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great location! I have stayed at this hotel on...</td>\n",
       "      <td>2019/08/20</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hotel was adequate for my stay. The strips...</td>\n",
       "      <td>2019/08/20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great location, room was large and spacious. P...</td>\n",
       "      <td>2019/08/19</td>\n",
       "      <td>Palm Harbor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review        date     Location\n",
       "0  I was very impressed with the resort.\\n Great ...  2019/08/20    Sebastian\n",
       "1  The rooms were nice the outside needs work als...  2019/08/20  Los Angeles\n",
       "2  Great location! I have stayed at this hotel on...  2019/08/20      Georgia\n",
       "3  The hotel was adequate for my stay. The strips...  2019/08/20          NaN\n",
       "4  Great location, room was large and spacious. P...  2019/08/19  Palm Harbor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Test Data.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad6335",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66094f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>date</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6393</td>\n",
       "      <td>6448</td>\n",
       "      <td>1711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6384</td>\n",
       "      <td>403</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Amazing\\n  \\n  Read more\\n  Read less</td>\n",
       "      <td>Jun 2019</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>145</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Review      date  \\\n",
       "count                                    6393      6448   \n",
       "unique                                   6384       403   \n",
       "top     Amazing\\n  \\n  Read more\\n  Read less  Jun 2019   \n",
       "freq                                        4       145   \n",
       "\n",
       "                        Location  \n",
       "count                       1711  \n",
       "unique                      1082  \n",
       "top     United States of America  \n",
       "freq                         116  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b89abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6448 entries, 0 to 6447\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Review    6393 non-null   object\n",
      " 1   date      6448 non-null   object\n",
      " 2   Location  1711 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 151.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd8778e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States of America    116\n",
       "New York                     34\n",
       "California                   19\n",
       "San Jose                     19\n",
       "Canada                       16\n",
       "                           ... \n",
       "Longwood                      1\n",
       "panama                        1\n",
       "RI                            1\n",
       "St. Louis Mo                  1\n",
       "Anaheim Ca                    1\n",
       "Name: Location, Length: 1082, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33d07b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jun 2019      145\n",
       "May 2019      121\n",
       "Jul 2019       92\n",
       "2018/12/01     66\n",
       "Oct 2018       58\n",
       "             ... \n",
       "2019/2/14       3\n",
       "2019/05/31      2\n",
       "Dec 2018        2\n",
       "2019/06/06      2\n",
       "2019/06/08      2\n",
       "Name: date, Length: 403, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9db179a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         6448\n",
       "unique         403\n",
       "top       Jun 2019\n",
       "freq           145\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fdcde05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                         1711\n",
       "unique                        1082\n",
       "top       United States of America\n",
       "freq                           116\n",
       "Name: Location, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Location'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5316518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The rooms were nice the outside needs work also no free breakfast it would have been nice overall it was ok\\n  \\n  Read more\\n  Read less'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bfe93b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Likes:\\n The bed was very comfortable. \\n \\n Dislikes: \\n Staff were not very friendly. The breakfast was not very good. \\n Our bed runner had some marks of burning with something like a cigarette.\\n  \\n  Read more\\n  Read less'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review'].values[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7d6aa",
   "metadata": {},
   "source": [
    "### Checking the Null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae2b0d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4737"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Location\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7f26d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1ae9b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2e798",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d306d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c59ce8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",'read',\"more\",'less'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e1c37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6448/6448 [00:05<00:00, 1164.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Applying all the text preprocessing steps:-\n",
    "data['Review'] = data['Review'].apply(str)\n",
    "from tqdm import tqdm\n",
    "preprocessed_review=[]\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(data['Review'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_review.append(sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ef1cd",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8568472",
   "metadata": {},
   "source": [
    "## Bag Of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7e35046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names  ['aaaaaaaaaaaaa', 'aamupalaa', 'aan', 'aand', 'aangegeven', 'aangevraagd', 'aanrader', 'aanwezig', 'aardig', 'ab']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (6448, 12218)\n",
      "the number of unique words  12218\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "count_vect.fit(preprocessed_review)\n",
    "print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "print('='*50)\n",
    "\n",
    "final_counts = count_vect.transform(preprocessed_reviews)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd831b",
   "metadata": {},
   "source": [
    "## Bi-Grams and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb989829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (6448, 2621)\n",
      "the number of unique words including both unigrams and bigrams  2621\n"
     ]
    }
   ],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "# count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# please do read the CountVectorizer documentation http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# you can choose these numebrs min_df=10, max_features=5000, of your choice\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "final_bigram_counts = count_vect.fit_transform(preprocessed_review)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caac28",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1611484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['aber', 'able', 'able check', 'able get', 'able walk', 'absolutely', 'ac', 'access', 'access subway', 'accessible']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (6448, 2621)\n",
      "the number of unique words including both unigrams and bigrams  2621\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_vect.fit(preprocessed_review)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "final_tf_idf = tf_idf_vect.transform(preprocessed_review)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3994f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
